import json
import os

# ë…¸íŠ¸ë¶ ë‚´ìš© ì •ì˜ (JSON êµ¬ì¡°)
notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Based Hybrid Text Analysis: Topic Modeling & Network Analysis\n",
    "## BERTopic, SNA, and LLM Prompting (Azure OpenAI)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì—°êµ¬ ë³´ê³ ì„œì—ì„œ ì œì•ˆí•œ **'AI ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„ì„ í”„ë ˆì„ì›Œí¬'**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¶„ì„ í”„ë¡œì„¸ìŠ¤:**\n",
    "1. **Data Loading & Preprocessing:** ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "2. **BERTopic Analysis:** ë™ì  í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì£¼ìš” ì´ìŠˆ ë° ì‹œê³„ì—´ ë³€í™” ì¶”ì \n",
    "3. **Semantic Network Analysis (SNA):** í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê¸°ìˆ  ìƒíƒœê³„ ì§€ë„)\n",
    "4. **LLM Prompt Generation:** ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ í•´ì„ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±\n",
    "5. **LLM Integration (Azure):** Azure OpenAIë¥¼ ì—°ë™í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Azure OpenAI ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê¸°ì¤€)\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "print(\"Libraries Loaded & Environment Variables Checked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading & Preprocessing\n",
    "def load_and_preprocess():\n",
    "    try:\n",
    "        sam_df = pd.read_csv(os.path.join(DATA_DIR, \"samsung_news.csv\"))\n",
    "        sk_df = pd.read_csv(os.path.join(DATA_DIR, \"skhynix_news.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        sam_df = pd.DataFrame({'date': ['2024-01-01'], 'title': ['ì‚¼ì„±ì „ì GAA íŒŒìš´ë“œë¦¬'], 'content': ['ì‚¼ì„±ì „ìê°€ 3ë‚˜ë…¸ GAA ê³µì • ì–‘ì‚°ì„ ì‹œì‘í–ˆë‹¤.']})\n",
    "        sk_df = pd.DataFrame({'date': ['2024-01-01'], 'title': ['SKí•˜ì´ë‹‰ìŠ¤ HBM3E'], 'content': ['SKí•˜ì´ë‹‰ìŠ¤ê°€ HBM3Eë¥¼ ì—”ë¹„ë””ì•„ì— ë…ì  ê³µê¸‰í•œë‹¤.']})\n",
    "    \n",
    "    sam_df['company'] = 'Samsung'\n",
    "    sk_df['company'] = 'SKHynix'\n",
    "    df = pd.concat([sam_df, sk_df], ignore_index=True)\n",
    "    df['text'] = df['title'].fillna('') + \" \" + df['content'].fillna('')\n",
    "    df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
    "    df = df.dropna(subset=['date']).sort_values('date')\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    df['processed_text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "df = load_and_preprocess()\n",
    "print(f\"Total Data: {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERTopic Analysis\n",
    "def run_bertopic(docs, min_topic_size=30):\n",
    "    print(\"Initializing BERTopic...\")\n",
    "    korean_stopwords = ['ê¸°ì', 'ë°°í¬', 'ê¸ˆì§€', 'ë¬´ë‹¨', 'ì „ì¬', 'ìœ„í•´', 'í†µí•´', 'ê´€ë ¨', 'ëŒ€í•œ', 'ì´ë²ˆ', 'ìˆë‹¤', 'í–ˆë‹¤', 'ë°í˜”ë‹¤']\n",
    "    vectorizer_model = CountVectorizer(stop_words=korean_stopwords)\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        min_topic_size=min_topic_size,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    return topic_model, topics\n",
    "\n",
    "sample_df = df.tail(1000).copy()\n",
    "docs = sample_df['processed_text'].tolist()\n",
    "\n",
    "try:\n",
    "    topic_model, topics = run_bertopic(docs)\n",
    "    print(\"BERTopic Training Complete.\")\n",
    "    freq = topic_model.get_topic_info(); \n",
    "    print(freq.head(5))\n",
    "except Exception as e:\n",
    "    print(f\"BERTopic Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Semantic Network Analysis (SNA)\n",
    "def extract_keywords_regex(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    tech_terms = re.findall(r'\\b[A-Za-z][A-Za-z0-9]+\\b', text)\n",
    "    korean_nouns = re.findall(r'[ê°€-í£]{2,}', text)\n",
    "    target_keywords = set(['HBM', 'DDR', 'NAND', 'AI', 'ë°˜ë„ì²´', 'íŒŒìš´ë“œë¦¬', 'ìˆ˜ìœ¨', 'íˆ¬ì', 'ì–‘ì‚°', \n",
    "                          'ì—”ë¹„ë””ì•„', 'TSMC', 'GAA', 'ì´ìµ', 'ì ì', 'CXL', 'PIM', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì‚¼ì„±ì „ì'])\n",
    "    found = []\n",
    "    for word in tech_terms + korean_nouns:\n",
    "        w_upper = word.upper()\n",
    "        for target in target_keywords:\n",
    "            if target in w_upper or w_upper in target:\n",
    "                found.append(target)\n",
    "                break\n",
    "    return list(set(found))\n",
    "\n",
    "def draw_semantic_network(df, company_name, threshold=5):\n",
    "    subset = df[df['company'] == company_name].copy()\n",
    "    subset['keywords'] = subset['processed_text'].apply(extract_keywords_regex)\n",
    "    edge_list = []\n",
    "    for keywords in subset['keywords']:\n",
    "        if len(keywords) > 1:\n",
    "            edge_list.extend(combinations(sorted(keywords), 2))\n",
    "    edge_counts = Counter(edge_list)\n",
    "    G = nx.Graph()\n",
    "    for (u, v), count in edge_counts.items():\n",
    "        if count >= threshold: G.add_edge(u, v, weight=count)\n",
    "    return G\n",
    "\n",
    "G_sk = draw_semantic_network(sample_df, \"SKHynix\", threshold=20)\n",
    "G_sam = draw_semantic_network(sample_df, \"Samsung\", threshold=20)\n",
    "print(\"Networks Built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LLM Prompt Generation\n",
    "def generate_llm_prompt(G, company_name):\n",
    "    degree_dict = dict(G.degree(weight='weight'))\n",
    "    top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_keywords = [k[0] for k in top_nodes]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    [ì—­í• ]\n",
    "    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µì°°ë ¥ ìˆëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
    "    \n",
    "    [ë¶„ì„ ë°ì´í„°: {company_name}]\n",
    "    1. í•µì‹¬ ì—°ê²° í‚¤ì›Œë“œ (Top 5): {top_keywords}\n",
    "    2. ë„¤íŠ¸ì›Œí¬ íŠ¹ì§•: '{top_keywords[0]}' í‚¤ì›Œë“œê°€ ì¤‘ì‹¬ í—ˆë¸Œ ì—­í• ì„ í•˜ë©° ê¸°ìˆ  ìƒíƒœê³„ë¥¼ ì£¼ë„í•¨.\n",
    "    \n",
    "    [ìš”ì²­ ì‚¬í•­]\n",
    "    1. **ì „ëµ ì§„ë‹¨:** í˜„ì¬ {company_name}ì˜ ê¸°ìˆ  ì „ëµì´ 'ì§‘ì¤‘í˜•'ì¸ì§€ 'ë¶„ì‚°í˜•'ì¸ì§€ ì§„ë‹¨í•´ ì£¼ì„¸ìš”.\n",
    "    2. **ì„±ê³¼ ì˜ˆì¸¡:** íŠ¹íˆ '{top_keywords[0]}' ê¸°ìˆ ì´ AI ì‚°ì—… íŠ¸ë Œë“œì™€ ê²°í•©í•˜ì—¬ í–¥í›„ ì–´ë–¤ ì¬ë¬´ì  ì„±ê³¼ë¥¼ ë‚¼ì§€ ì¶”ë¡ í•´ ì£¼ì„¸ìš”.\n",
    "    3. **ì•½í•œ ì‹ í˜¸(Weak Signal):** ë„¤íŠ¸ì›Œí¬ ì£¼ë³€ë¶€ì— ìœ„ì¹˜í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì°¨ì„¸ëŒ€ ê¸°ìˆ ì„ ì œì•ˆí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "target_prompt = generate_llm_prompt(G_sk, \"SKí•˜ì´ë‹‰ìŠ¤\")\n",
    "print(target_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Azure OpenAI Integration\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©, ì‹¤ì œ í‚¤ ê°’ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ)\n",
    "print(\"Azure OpenAI Config Check:\")\n",
    "print(f\"- Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"- Deployment Name: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")\n",
    "print(f\"- API Version: {os.getenv('AZURE_OPENAI_API_VERSION')}\")\n",
    "\n",
    "def query_azure_llm(prompt):\n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    )\n",
    "    \n",
    "    deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\") # e.g., 'gpt-4o'\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¤– Azure LLMì—ê²Œ ë¶„ì„ ìš”ì²­ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled in semiconductor industry analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error calling Azure OpenAI: {e}\"\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ (í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì‹¤í–‰ë¨)\n",
    "if os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    analysis_result = query_azure_llm(target_prompt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*20 + \" Azure LLM ë¶„ì„ ë¦¬í¬íŠ¸ \" + \"=\"*20 + \"\\n\")\n",
    "    print(analysis_result)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ .env íŒŒì¼ì— AZURE_OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# íŒŒì¼ ìƒì„±
output_path = "/home/arkwith/SKKU/tech_forcast/notebooks/02_advanced_analysis.ipynb"
os.makedirs(os.path.dirname(output_path), exist_ok=True)

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1, ensure_ascii=False)

print(f"Notebook updated at {output_path}")
