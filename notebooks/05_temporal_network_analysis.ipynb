{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Temporal Semantic Network Analysis (시계열 의미 연결망 분석)\n",
        "## 2014-2024년 연도별 토픽 진화 및 네트워크 구조 변화 추적\n",
        "\n",
        "이 노트북은 단순한 정적 네트워크가 아닌 **시간에 따른 네트워크 진화**를 분석합니다.\n",
        "\n",
        "**분석 내용:**\n",
        "1. 연도별 키워드 네트워크 생성 (2014~2024)\n",
        "2. 시계열 중심성 변화 추적 (Degree Centrality Evolution)\n",
        "3. 토픽 전환 경로 시각화 (DRAM → HBM 등)\n",
        "4. 기업별 전략 진화 비교 (Samsung vs SK Hynix)\n",
        "5. 신규 등장 키워드 탐지 (Emerging Topics)\n",
        "6. 연도별 네트워크 시각화 (Small Multiples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 시각화 설정\n",
        "sns.set(style=\"whitegrid\")\n",
        "font_name = \"NanumGothic\"\n",
        "plt.rcParams[\"font.family\"] = font_name\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "# 데이터 경로\n",
        "DATA_DIR = \"../data/raw\"\n",
        "OUTPUT_DIR = \"../reports/Figure\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Environment Setup Complete. Using font: {font_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 데이터 로딩 및 전처리\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess():\n",
        "    \"\"\"뉴스 데이터 로드 및 연도 정보 추가\"\"\"\n",
        "    sam_df = pd.read_csv(os.path.join(DATA_DIR, \"samsung_news.csv\"))\n",
        "    sk_df = pd.read_csv(os.path.join(DATA_DIR, \"skhynix_news.csv\"))\n",
        "    \n",
        "    sam_df['company'] = 'Samsung'\n",
        "    sk_df['company'] = 'SKHynix'\n",
        "    \n",
        "    df = pd.concat([sam_df, sk_df], ignore_index=True)\n",
        "    df['text'] = df['title'].fillna('') + \" \" + df['content'].fillna('')\n",
        "    df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
        "    df = df.dropna(subset=['date']).sort_values('date')\n",
        "    \n",
        "    # 연도 추출\n",
        "    df['year'] = df['date'].dt.year\n",
        "    \n",
        "    # 2014-2024년 필터링\n",
        "    df = df[(df['year'] >= 2014) & (df['year'] <= 2024)]\n",
        "    \n",
        "    def clean_text(text):\n",
        "        text = str(text)\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    \n",
        "    df['processed_text'] = df['text'].apply(clean_text)\n",
        "    return df\n",
        "\n",
        "df = load_and_preprocess()\n",
        "print(f\"Total Data: {len(df)} records\")\n",
        "print(f\"Years: {sorted(df['year'].unique())}\")\n",
        "print(f\"\\nData by Year:\")\n",
        "print(df.groupby(['year', 'company']).size().unstack(fill_value=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 키워드 추출\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_keywords_advanced(text):\n",
        "    \"\"\"향상된 키워드 추출: 기술, 제품, 응용 분야 모두 포함\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    \n",
        "    # 확장된 키워드 세트\n",
        "    target_keywords = [\n",
        "        # 메모리 기술\n",
        "        'HBM', 'HBM2', 'HBM3', 'HBM3E',\n",
        "        'DDR', 'DDR4', 'DDR5', 'LPDDR',\n",
        "        'DRAM', 'NAND', 'SSD', 'V-NAND',\n",
        "        \n",
        "        # 제조 기술\n",
        "        'EUV', 'GAA', '파운드리', '패키징',\n",
        "        '10나노', '7나노', '5나노', '3나노', '2나노',\n",
        "        \n",
        "        # 응용 분야\n",
        "        'AI', '인공지능', '머신러닝', 'GPU',\n",
        "        '서버', '데이터센터', '클라우드',\n",
        "        '자율주행', '전기차', '차량용',\n",
        "        '스마트폰', '모바일', '5G',\n",
        "        \n",
        "        # 기업/파트너\n",
        "        '엔비디아', 'NVIDIA', 'AMD', 'Intel',\n",
        "        'TSMC', '삼성전자', 'SK하이닉스',\n",
        "        \n",
        "        # 비즈니스\n",
        "        '양산', '개발', '출시', '공급',\n",
        "        '투자', '매출', '수율', '점유율',\n",
        "        \n",
        "        # 차세대 기술\n",
        "        'CXL', 'PIM', 'CIS', 'AP',\n",
        "        '하이브리드본딩', '3D', 'TSV'\n",
        "    ]\n",
        "    \n",
        "    found = set()\n",
        "    text_upper = text.upper()\n",
        "    \n",
        "    for keyword in target_keywords:\n",
        "        keyword_upper = keyword.upper()\n",
        "        if keyword_upper in text_upper or keyword in text:\n",
        "            found.add(keyword)\n",
        "    \n",
        "    return list(found)\n",
        "\n",
        "# 전체 데이터에 키워드 추출 적용\n",
        "df['keywords'] = df['processed_text'].apply(extract_keywords_advanced)\n",
        "df['keyword_count'] = df['keywords'].apply(len)\n",
        "\n",
        "print(\"Keyword extraction completed.\")\n",
        "print(f\"Articles with keywords: {len(df[df['keyword_count'] > 0])} / {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 연도별 네트워크 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_yearly_networks(df, company_filter=None, min_edge_weight=3):\n",
        "    \"\"\"\n",
        "    연도별 네트워크 생성\n",
        "    \n",
        "    Parameters:\n",
        "    - df: 전체 데이터프레임\n",
        "    - company_filter: 'Samsung', 'SKHynix', 또는 None (전체)\n",
        "    - min_edge_weight: 최소 공출현 횟수\n",
        "    \n",
        "    Returns:\n",
        "    - dict: {year: networkx.Graph}\n",
        "    \"\"\"\n",
        "    if company_filter:\n",
        "        df = df[df['company'] == company_filter].copy()\n",
        "    \n",
        "    yearly_networks = {}\n",
        "    years = sorted(df['year'].unique())\n",
        "    \n",
        "    for year in years:\n",
        "        year_df = df[df['year'] == year]\n",
        "        \n",
        "        # 키워드 공출현 계산\n",
        "        edge_list = []\n",
        "        for keywords in year_df['keywords']:\n",
        "            if len(keywords) > 1:\n",
        "                edge_list.extend(combinations(sorted(keywords), 2))\n",
        "        \n",
        "        edge_counts = Counter(edge_list)\n",
        "        \n",
        "        # 네트워크 구축\n",
        "        G = nx.Graph()\n",
        "        for (u, v), count in edge_counts.items():\n",
        "            if count >= min_edge_weight:\n",
        "                G.add_edge(u, v, weight=count)\n",
        "        \n",
        "        yearly_networks[year] = G\n",
        "        print(f\"Year {year}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    \n",
        "    return yearly_networks\n",
        "\n",
        "# 전체 데이터 네트워크\n",
        "print(\"=== Building Yearly Networks (All Companies) ===\")\n",
        "networks_all = build_yearly_networks(df, company_filter=None, min_edge_weight=5)\n",
        "\n",
        "# 기업별 네트워크\n",
        "print(\"\\n=== Building Yearly Networks (Samsung) ===\")\n",
        "networks_samsung = build_yearly_networks(df, company_filter='Samsung', min_edge_weight=3)\n",
        "\n",
        "print(\"\\n=== Building Yearly Networks (SK Hynix) ===\")\n",
        "networks_skhynix = build_yearly_networks(df, company_filter='SKHynix', min_edge_weight=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 중심성 시계열 분석\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_centrality_evolution(networks_dict):\n",
        "    \"\"\"\n",
        "    연도별 중심성 변화 추적\n",
        "    \n",
        "    Returns:\n",
        "    - DataFrame: 연도별 주요 키워드의 중심성 점수\n",
        "    \"\"\"\n",
        "    centrality_data = []\n",
        "    \n",
        "    for year, G in sorted(networks_dict.items()):\n",
        "        if G.number_of_nodes() == 0:\n",
        "            continue\n",
        "        \n",
        "        # Degree Centrality 계산\n",
        "        degree_cent = nx.degree_centrality(G)\n",
        "        \n",
        "        # 가중치 반영한 Weighted Degree\n",
        "        weighted_degree = dict(G.degree(weight='weight'))\n",
        "        \n",
        "        for node in G.nodes():\n",
        "            centrality_data.append({\n",
        "                'year': year,\n",
        "                'keyword': node,\n",
        "                'degree_centrality': degree_cent.get(node, 0),\n",
        "                'weighted_degree': weighted_degree.get(node, 0)\n",
        "            })\n",
        "    \n",
        "    df_cent = pd.DataFrame(centrality_data)\n",
        "    return df_cent\n",
        "\n",
        "# 전체 중심성 분석\n",
        "centrality_all = analyze_centrality_evolution(networks_all)\n",
        "centrality_samsung = analyze_centrality_evolution(networks_samsung)\n",
        "centrality_skhynix = analyze_centrality_evolution(networks_skhynix)\n",
        "\n",
        "print(\"Centrality evolution analysis completed.\")\n",
        "print(f\"\\nTotal keyword-year pairs: {len(centrality_all)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 주요 키워드 중심성 변화 시각화\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_keyword_evolution(df_cent, keywords_to_track, title=\"Keyword Centrality Evolution\"):\n",
        "    \"\"\"특정 키워드들의 중심성 변화를 시계열 그래프로 표현\"\"\"\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    \n",
        "    for keyword in keywords_to_track:\n",
        "        subset = df_cent[df_cent['keyword'] == keyword].sort_values('year')\n",
        "        if len(subset) > 0:\n",
        "            plt.plot(subset['year'], subset['weighted_degree'], \n",
        "                    marker='o', linewidth=2.5, markersize=8, label=keyword)\n",
        "    \n",
        "    plt.title(title, fontsize=18, fontweight='bold')\n",
        "    plt.xlabel(\"Year\", fontsize=14)\n",
        "    plt.ylabel(\"Weighted Degree (Network Centrality)\", fontsize=14)\n",
        "    plt.legend(loc='best', fontsize=11, ncol=2)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 주요 기술 키워드 추적\n",
        "tech_keywords = ['HBM', 'DRAM', 'DDR', 'NAND', 'AI', '파운드리', 'EUV', 'GAA']\n",
        "plot_keyword_evolution(centrality_all, tech_keywords, \n",
        "                      title=\"반도체 핵심 기술 키워드의 네트워크 중심성 변화 (2014-2024)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 연도별 Top 키워드 히트맵\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_temporal_heatmap(df_cent, top_n=15):\n",
        "    \"\"\"연도별 주요 키워드의 중심성을 히트맵으로 시각화\"\"\"\n",
        "    # 전체 기간에서 가장 중요한 키워드 선택\n",
        "    top_keywords = (df_cent.groupby('keyword')['weighted_degree']\n",
        "                    .sum()\n",
        "                    .sort_values(ascending=False)\n",
        "                    .head(top_n)\n",
        "                    .index.tolist())\n",
        "    \n",
        "    # 피벗 테이블 생성\n",
        "    pivot_data = df_cent[df_cent['keyword'].isin(top_keywords)].pivot_table(\n",
        "        index='keyword',\n",
        "        columns='year',\n",
        "        values='weighted_degree',\n",
        "        fill_value=0\n",
        "    )\n",
        "    \n",
        "    # 키워드를 전체 중심성 합계로 정렬\n",
        "    pivot_data['total'] = pivot_data.sum(axis=1)\n",
        "    pivot_data = pivot_data.sort_values('total', ascending=False).drop('total', axis=1)\n",
        "    \n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(pivot_data, annot=True, fmt='.0f', cmap='YlOrRd', \n",
        "                linewidths=0.5, cbar_kws={'label': 'Weighted Degree'})\n",
        "    plt.title('연도별 핵심 키워드 네트워크 중심성 히트맵 (2014-2024)', \n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Year', fontsize=13)\n",
        "    plt.ylabel('Keyword', fontsize=13)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return pivot_data\n",
        "\n",
        "heatmap_data = create_temporal_heatmap(centrality_all, top_n=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 기업별 전략 진화 비교\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_company_comparison(cent_samsung, cent_skhynix, keywords):\n",
        "    \"\"\"삼성전자 vs SK하이닉스의 키워드 중심성 비교\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, keyword in enumerate(keywords[:4]):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # 삼성 데이터\n",
        "        sam_data = cent_samsung[cent_samsung['keyword'] == keyword].sort_values('year')\n",
        "        if len(sam_data) > 0:\n",
        "            ax.plot(sam_data['year'], sam_data['weighted_degree'], \n",
        "                   marker='s', linewidth=3, markersize=10, label='Samsung', color='#1f77b4')\n",
        "        \n",
        "        # SK하이닉스 데이터\n",
        "        sk_data = cent_skhynix[cent_skhynix['keyword'] == keyword].sort_values('year')\n",
        "        if len(sk_data) > 0:\n",
        "            ax.plot(sk_data['year'], sk_data['weighted_degree'], \n",
        "                   marker='o', linewidth=3, markersize=10, label='SK Hynix', color='#ff7f0e')\n",
        "        \n",
        "        ax.set_title(f'{keyword} 키워드 중심성 비교', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Year', fontsize=11)\n",
        "        ax.set_ylabel('Weighted Degree', fontsize=11)\n",
        "        ax.legend(fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('삼성전자 vs SK하이닉스: 핵심 키워드 전략 진화 비교', \n",
        "                fontsize=18, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "comparison_keywords = ['HBM', 'DRAM', 'AI', '파운드리']\n",
        "plot_company_comparison(centrality_samsung, centrality_skhynix, comparison_keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 토픽 전환 경로 분석 (Topic Transition)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_topic_transition(df_cent, keyword_pairs):\n",
        "    \"\"\"\n",
        "    특정 키워드 쌍의 중심성 역전 시점 분석\n",
        "    예: DRAM에서 HBM으로의 전환\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(len(keyword_pairs), 1, figsize=(16, 5*len(keyword_pairs)))\n",
        "    if len(keyword_pairs) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, (old_topic, new_topic) in enumerate(keyword_pairs):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        old_data = df_cent[df_cent['keyword'] == old_topic].sort_values('year')\n",
        "        new_data = df_cent[df_cent['keyword'] == new_topic].sort_values('year')\n",
        "        \n",
        "        if len(old_data) > 0:\n",
        "            ax.plot(old_data['year'], old_data['weighted_degree'], \n",
        "                   marker='o', linewidth=3, markersize=10, label=f'{old_topic} (레거시)', \n",
        "                   color='#8c564b', linestyle='--')\n",
        "        \n",
        "        if len(new_data) > 0:\n",
        "            ax.plot(new_data['year'], new_data['weighted_degree'], \n",
        "                   marker='s', linewidth=3, markersize=10, label=f'{new_topic} (신규)', \n",
        "                   color='#e377c2')\n",
        "        \n",
        "        # 교차점 찾기\n",
        "        if len(old_data) > 0 and len(new_data) > 0:\n",
        "            merged = pd.merge(old_data[['year', 'weighted_degree']], \n",
        "                            new_data[['year', 'weighted_degree']], \n",
        "                            on='year', suffixes=('_old', '_new'))\n",
        "            merged['diff'] = merged['weighted_degree_new'] - merged['weighted_degree_old']\n",
        "            \n",
        "            crossover = merged[merged['diff'] > 0]\n",
        "            if len(crossover) > 0:\n",
        "                crossover_year = crossover.iloc[0]['year']\n",
        "                ax.axvline(crossover_year, color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
        "                ax.text(crossover_year, ax.get_ylim()[1]*0.9, \n",
        "                       f'전환점: {int(crossover_year)}년', \n",
        "                       ha='center', fontsize=12, color='red', fontweight='bold',\n",
        "                       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
        "        \n",
        "        ax.set_title(f'토픽 전환 분석: {old_topic} → {new_topic}', \n",
        "                    fontsize=15, fontweight='bold')\n",
        "        ax.set_xlabel('Year', fontsize=12)\n",
        "        ax.set_ylabel('Network Centrality', fontsize=12)\n",
        "        ax.legend(fontsize=11, loc='best')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 주요 토픽 전환 쌍\n",
        "transition_pairs = [\n",
        "    ('DRAM', 'HBM'),\n",
        "]\n",
        "\n",
        "analyze_topic_transition(centrality_all, transition_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 연도별 네트워크 시각화 (Small Multiples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_temporal_networks_by_company(networks_dict, company_name, years_to_plot=None, top_nodes=15):\n",
        "    \"\"\"\n",
        "    특정 회사의 연도별 네트워크를 Small Multiples로 시각화\n",
        "    \n",
        "    Parameters:\n",
        "    - networks_dict: 연도별 네트워크 딕셔너리\n",
        "    - company_name: 회사명 (표시용)\n",
        "    - years_to_plot: 시각화할 연도 리스트 (None이면 전체)\n",
        "    - top_nodes: 각 네트워크에서 표시할 최대 노드 수\n",
        "    \"\"\"\n",
        "    if years_to_plot is None:\n",
        "        years_to_plot = sorted(networks_dict.keys())\n",
        "    \n",
        "    n_years = len(years_to_plot)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_years + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(22, 6*n_rows))\n",
        "    axes = axes.flatten() if n_years > 1 else [axes]\n",
        "    \n",
        "    for idx, year in enumerate(years_to_plot):\n",
        "        ax = axes[idx]\n",
        "        G = networks_dict[year]\n",
        "        \n",
        "        if G.number_of_nodes() == 0:\n",
        "            ax.text(0.5, 0.5, f'{year}년\\n(데이터 없음)', \n",
        "                   ha='center', va='center', fontsize=14, color='gray')\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        \n",
        "        # 상위 노드만 선택 (Weighted Degree 기준)\n",
        "        degree_dict = dict(G.degree(weight='weight'))\n",
        "        top_nodes_list = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:top_nodes]\n",
        "        top_node_names = [node for node, _ in top_nodes_list]\n",
        "        \n",
        "        # 서브그래프 생성\n",
        "        G_sub = G.subgraph(top_node_names).copy()\n",
        "        \n",
        "        # 레이아웃 (Spring layout)\n",
        "        pos = nx.spring_layout(G_sub, k=1.0, iterations=50, seed=42)\n",
        "        \n",
        "        # 노드 크기 (degree에 비례)\n",
        "        node_sizes = [degree_dict[node] * 20 for node in G_sub.nodes()]\n",
        "        \n",
        "        # 노드 색상 (중심성에 따라)\n",
        "        node_colors = [degree_dict[node] for node in G_sub.nodes()]\n",
        "        \n",
        "        # 엣지 두께 (weight에 비례)\n",
        "        edges = G_sub.edges()\n",
        "        weights = [G_sub[u][v]['weight'] for u, v in edges]\n",
        "        max_weight = max(weights) if weights else 1\n",
        "        edge_widths = [w / max_weight * 3 for w in weights]\n",
        "        \n",
        "        # 네트워크 그리기\n",
        "        nx.draw_networkx_nodes(G_sub, pos, node_size=node_sizes, \n",
        "                              node_color=node_colors, cmap='YlOrRd',\n",
        "                              alpha=0.9, ax=ax, vmin=0, vmax=max(node_colors) if node_colors else 1)\n",
        "        \n",
        "        nx.draw_networkx_edges(G_sub, pos, width=edge_widths, \n",
        "                              alpha=0.4, edge_color='gray', ax=ax)\n",
        "        \n",
        "        # 레이블 (주요 노드만)\n",
        "        top_5_nodes = [node for node, _ in top_nodes_list[:5]]\n",
        "        labels = {node: node for node in G_sub.nodes() if node in top_5_nodes}\n",
        "        nx.draw_networkx_labels(G_sub, pos, labels=labels, font_size=9, \n",
        "                               font_weight='bold', font_family='NanumGothic', ax=ax)\n",
        "        \n",
        "        # 제목\n",
        "        ax.set_title(f'{year}년\\n노드: {G.number_of_nodes()} | 엣지: {G.number_of_edges()}', \n",
        "                    fontsize=12, fontweight='bold', pad=10)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    # 남은 빈 서브플롯 제거\n",
        "    for idx in range(n_years, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'{company_name} 연도별 키워드 네트워크 진화 (2014-2024)', \n",
        "                fontsize=20, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # 저장\n",
        "    save_path = os.path.join(OUTPUT_DIR, f'fig_12_temporal_network_{company_name}.png')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# 삼성전자 연도별 네트워크\n",
        "print(\"=== Samsung Electronics Yearly Networks ===\")\n",
        "plot_temporal_networks_by_company(networks_samsung, \"Samsung\", top_nodes=12)\n",
        "\n",
        "# SK하이닉스 연도별 네트워크\n",
        "print(\"\\n=== SK Hynix Yearly Networks ===\")\n",
        "plot_temporal_networks_by_company(networks_skhynix, \"SKHynix\", top_nodes=12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 연도별 키워드 워드클라우드 (Word Cloud Style)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_wordcloud_by_year(networks_dict, company_name, years_to_plot=None):\n",
        "    \"\"\"\n",
        "    연도별 키워드를 워드클라우드 스타일로 시각화\n",
        "    키워드 크기 = 네트워크 중심성(Weighted Degree)\n",
        "    \"\"\"\n",
        "    from wordcloud import WordCloud\n",
        "    \n",
        "    if years_to_plot is None:\n",
        "        years_to_plot = sorted(networks_dict.keys())\n",
        "    \n",
        "    n_years = len(years_to_plot)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_years + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "    axes = axes.flatten() if n_years > 1 else [axes]\n",
        "    \n",
        "    for idx, year in enumerate(years_to_plot):\n",
        "        ax = axes[idx]\n",
        "        G = networks_dict[year]\n",
        "        \n",
        "        if G.number_of_nodes() == 0:\n",
        "            ax.text(0.5, 0.5, f'{year}년\\n(데이터 없음)', \n",
        "                   ha='center', va='center', fontsize=14, color='gray')\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        \n",
        "        # Weighted Degree를 단어 빈도처럼 사용\n",
        "        degree_dict = dict(G.degree(weight='weight'))\n",
        "        \n",
        "        # WordCloud 생성\n",
        "        try:\n",
        "            wc = WordCloud(\n",
        "                width=600, \n",
        "                height=400,\n",
        "                background_color='white',\n",
        "                colormap='YlOrRd',\n",
        "                font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
        "                relative_scaling=0.5,\n",
        "                min_font_size=8\n",
        "            ).generate_from_frequencies(degree_dict)\n",
        "            \n",
        "            ax.imshow(wc, interpolation='bilinear')\n",
        "            ax.set_title(f'{year}년 (키워드: {G.number_of_nodes()}개)', \n",
        "                        fontsize=13, fontweight='bold')\n",
        "            ax.axis('off')\n",
        "        except Exception as e:\n",
        "            ax.text(0.5, 0.5, f'{year}년\\n워드클라우드 생성 실패', \n",
        "                   ha='center', va='center', fontsize=12, color='red')\n",
        "            ax.axis('off')\n",
        "    \n",
        "    # 남은 빈 서브플롯 제거\n",
        "    for idx in range(n_years, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'{company_name} 연도별 키워드 워드클라우드 (2014-2024)', \n",
        "                fontsize=20, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # 저장\n",
        "    save_path = os.path.join(OUTPUT_DIR, f'fig_13_wordcloud_{company_name}.png')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# 삼성전자 워드클라우드\n",
        "print(\"=== Samsung Electronics Yearly Word Clouds ===\")\n",
        "plot_wordcloud_by_year(networks_samsung, \"Samsung\")\n",
        "\n",
        "# SK하이닉스 워드클라우드\n",
        "print(\"\\n=== SK Hynix Yearly Word Clouds ===\")\n",
        "plot_wordcloud_by_year(networks_skhynix, \"SKHynix\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. 특정 연도 비교 (Side-by-Side)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_networks_by_year(networks_samsung, networks_skhynix, years_to_compare):\n",
        "    \"\"\"\n",
        "    특정 연도들에 대해 삼성 vs SK하이닉스 네트워크를 나란히 비교\n",
        "    \"\"\"\n",
        "    n_years = len(years_to_compare)\n",
        "    fig, axes = plt.subplots(n_years, 2, figsize=(16, 7*n_years))\n",
        "    \n",
        "    if n_years == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for idx, year in enumerate(years_to_compare):\n",
        "        # 삼성 네트워크\n",
        "        ax_sam = axes[idx, 0]\n",
        "        G_sam = networks_samsung.get(year, nx.Graph())\n",
        "        \n",
        "        if G_sam.number_of_nodes() > 0:\n",
        "            degree_dict_sam = dict(G_sam.degree(weight='weight'))\n",
        "            top_nodes_sam = sorted(degree_dict_sam.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "            top_node_names_sam = [node for node, _ in top_nodes_sam]\n",
        "            \n",
        "            G_sub_sam = G_sam.subgraph(top_node_names_sam).copy()\n",
        "            pos_sam = nx.spring_layout(G_sub_sam, k=1.0, iterations=50, seed=42)\n",
        "            \n",
        "            node_sizes_sam = [degree_dict_sam[node] * 25 for node in G_sub_sam.nodes()]\n",
        "            node_colors_sam = [degree_dict_sam[node] for node in G_sub_sam.nodes()]\n",
        "            \n",
        "            edges_sam = G_sub_sam.edges()\n",
        "            weights_sam = [G_sub_sam[u][v]['weight'] for u, v in edges_sam]\n",
        "            edge_widths_sam = [w * 0.5 for w in weights_sam]\n",
        "            \n",
        "            nx.draw_networkx_nodes(G_sub_sam, pos_sam, node_size=node_sizes_sam,\n",
        "                                  node_color=node_colors_sam, cmap='Blues',\n",
        "                                  alpha=0.9, ax=ax_sam)\n",
        "            nx.draw_networkx_edges(G_sub_sam, pos_sam, width=edge_widths_sam,\n",
        "                                  alpha=0.4, edge_color='gray', ax=ax_sam)\n",
        "            \n",
        "            top_5_sam = [node for node, _ in top_nodes_sam[:5]]\n",
        "            labels_sam = {node: node for node in G_sub_sam.nodes() if node in top_5_sam}\n",
        "            nx.draw_networkx_labels(G_sub_sam, pos_sam, labels=labels_sam,\n",
        "                                   font_size=10, font_weight='bold', \n",
        "                                   font_family='NanumGothic', ax=ax_sam)\n",
        "        \n",
        "        ax_sam.set_title(f'삼성전자 {year}년\\n(노드: {G_sam.number_of_nodes()}, 엣지: {G_sam.number_of_edges()})',\n",
        "                        fontsize=14, fontweight='bold', color='#1f77b4')\n",
        "        ax_sam.axis('off')\n",
        "        \n",
        "        # SK하이닉스 네트워크\n",
        "        ax_sk = axes[idx, 1]\n",
        "        G_sk = networks_skhynix.get(year, nx.Graph())\n",
        "        \n",
        "        if G_sk.number_of_nodes() > 0:\n",
        "            degree_dict_sk = dict(G_sk.degree(weight='weight'))\n",
        "            top_nodes_sk = sorted(degree_dict_sk.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "            top_node_names_sk = [node for node, _ in top_nodes_sk]\n",
        "            \n",
        "            G_sub_sk = G_sk.subgraph(top_node_names_sk).copy()\n",
        "            pos_sk = nx.spring_layout(G_sub_sk, k=1.0, iterations=50, seed=42)\n",
        "            \n",
        "            node_sizes_sk = [degree_dict_sk[node] * 25 for node in G_sub_sk.nodes()]\n",
        "            node_colors_sk = [degree_dict_sk[node] for node in G_sub_sk.nodes()]\n",
        "            \n",
        "            edges_sk = G_sub_sk.edges()\n",
        "            weights_sk = [G_sub_sk[u][v]['weight'] for u, v in edges_sk]\n",
        "            edge_widths_sk = [w * 0.5 for w in weights_sk]\n",
        "            \n",
        "            nx.draw_networkx_nodes(G_sub_sk, pos_sk, node_size=node_sizes_sk,\n",
        "                                  node_color=node_colors_sk, cmap='Oranges',\n",
        "                                  alpha=0.9, ax=ax_sk)\n",
        "            nx.draw_networkx_edges(G_sub_sk, pos_sk, width=edge_widths_sk,\n",
        "                                  alpha=0.4, edge_color='gray', ax=ax_sk)\n",
        "            \n",
        "            top_5_sk = [node for node, _ in top_nodes_sk[:5]]\n",
        "            labels_sk = {node: node for node in G_sub_sk.nodes() if node in top_5_sk}\n",
        "            nx.draw_networkx_labels(G_sub_sk, pos_sk, labels=labels_sk,\n",
        "                                   font_size=10, font_weight='bold', \n",
        "                                   font_family='NanumGothic', ax=ax_sk)\n",
        "        \n",
        "        ax_sk.set_title(f'SK하이닉스 {year}년\\n(노드: {G_sk.number_of_nodes()}, 엣지: {G_sk.number_of_edges()})',\n",
        "                       fontsize=14, fontweight='bold', color='#ff7f0e')\n",
        "        ax_sk.axis('off')\n",
        "    \n",
        "    plt.suptitle('삼성전자 vs SK하이닉스: 연도별 네트워크 비교',\n",
        "                fontsize=18, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # 저장\n",
        "    save_path = os.path.join(OUTPUT_DIR, 'fig_14_network_comparison_by_year.png')\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Saved: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# 주요 연도 비교 (초기, 중기, 최근)\n",
        "key_years = [2016, 2019, 2022, 2024]\n",
        "print(f\"=== Comparing Networks for Key Years: {key_years} ===\")\n",
        "compare_networks_by_year(networks_samsung, networks_skhynix, key_years)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 종합 인사이트 리포트\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"시계열 네트워크 분석 종합 리포트\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. 전체 기간 통계\n",
        "print(\"\\n[1] 전체 분석 기간 통계\")\n",
        "print(f\"  - 분석 기간: {df['year'].min()}년 ~ {df['year'].max()}년\")\n",
        "print(f\"  - 총 기사 수: {len(df):,}건\")\n",
        "print(f\"  - 총 추출 키워드 수: {df['keywords'].apply(len).sum():,}개\")\n",
        "\n",
        "# 2. 연도별 네트워크 규모 변화\n",
        "print(\"\\n[2] 연도별 네트워크 규모 변화\")\n",
        "network_stats = []\n",
        "for year, G in sorted(networks_all.items()):\n",
        "    network_stats.append({\n",
        "        'Year': year,\n",
        "        'Nodes': G.number_of_nodes(),\n",
        "        'Edges': G.number_of_edges(),\n",
        "        'Density': round(nx.density(G), 3) if G.number_of_nodes() > 1 else 0\n",
        "    })\n",
        "df_stats = pd.DataFrame(network_stats)\n",
        "print(df_stats.to_string(index=False))\n",
        "\n",
        "# 3. 시기별 Top 키워드\n",
        "print(\"\\n[3] 시기별 Top 3 핵심 키워드\")\n",
        "periods = [\n",
        "    (2014, 2017, '초기'),\n",
        "    (2018, 2020, '중기'),\n",
        "    (2021, 2024, '최근')\n",
        "]\n",
        "\n",
        "for start, end, label in periods:\n",
        "    period_data = centrality_all[\n",
        "        (centrality_all['year'] >= start) & (centrality_all['year'] <= end)\n",
        "    ]\n",
        "    top_keywords = (period_data.groupby('keyword')['weighted_degree']\n",
        "                   .sum()\n",
        "                   .sort_values(ascending=False)\n",
        "                   .head(3))\n",
        "    print(f\"  {label} ({start}-{end}): {', '.join(top_keywords.index.tolist())}\")\n",
        "\n",
        "# 4. 기업별 전략 특징\n",
        "print(\"\\n[4] 기업별 전략 특징 (최근 3년 기준)\")\n",
        "recent_samsung = centrality_samsung[centrality_samsung['year'] >= 2022]\n",
        "recent_skhynix = centrality_skhynix[centrality_skhynix['year'] >= 2022]\n",
        "\n",
        "top_samsung = (recent_samsung.groupby('keyword')['weighted_degree']\n",
        "              .sum().sort_values(ascending=False).head(5))\n",
        "top_skhynix = (recent_skhynix.groupby('keyword')['weighted_degree']\n",
        "              .sum().sort_values(ascending=False).head(5))\n",
        "\n",
        "print(f\"  삼성전자 Top 5: {', '.join(top_samsung.index.tolist())}\")\n",
        "print(f\"  SK하이닉스 Top 5: {', '.join(top_skhynix.index.tolist())}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"분석 완료!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
