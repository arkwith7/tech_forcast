{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Based Hybrid Text Analysis: Topic Modeling & Network Analysis\n",
    "## BERTopic, SNA, and LLM Prompting (Azure OpenAI)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì—°êµ¬ ë³´ê³ ì„œì—ì„œ ì œì•ˆí•œ **'AI ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ í…ìŠ¤íŠ¸ ë¶„ì„ í”„ë ˆì„ì›Œí¬'**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¶„ì„ í”„ë¡œì„¸ìŠ¤:**\n",
    "1. **Data Loading & Preprocessing:** ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "2. **BERTopic Analysis:** ë™ì  í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì£¼ìš” ì´ìŠˆ ë° ì‹œê³„ì—´ ë³€í™” ì¶”ì \n",
    "3. **Semantic Network Analysis (SNA):** í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ê¸°ìˆ  ìƒíƒœê³„ ì§€ë„)\n",
    "4. **LLM Prompt Generation:** ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ í•´ì„ì„ ìœ„í•œ LLM í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±\n",
    "5. **LLM Integration (Azure):** Azure OpenAIë¥¼ ì—°ë™í•˜ì—¬ ì‹¤ì œ ë¶„ì„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded & Environment Variables Checked. Using font: NanumGothic\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Azure OpenAI ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "sns.set(style=\"whitegrid\")\n",
    "font_name = \"NanumGothic\"\n",
    "plt.rcParams[\"font.family\"] = font_name\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ê¸°ì¤€)\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "print(f\"Libraries Loaded & Environment Variables Checked. Using font: {font_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 2956 records\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading & Preprocessing\n",
    "def load_and_preprocess():\n",
    "    try:\n",
    "        sam_df = pd.read_csv(os.path.join(DATA_DIR, \"samsung_news.csv\"))\n",
    "        sk_df = pd.read_csv(os.path.join(DATA_DIR, \"skhynix_news.csv\"))\n",
    "    except FileNotFoundError:\n",
    "        print(\"ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        sam_df = pd.DataFrame({'date': ['2024-01-01'], 'title': ['ì‚¼ì„±ì „ì GAA íŒŒìš´ë“œë¦¬'], 'content': ['ì‚¼ì„±ì „ìê°€ 3ë‚˜ë…¸ GAA ê³µì • ì–‘ì‚°ì„ ì‹œì‘í–ˆë‹¤.']})\n",
    "        sk_df = pd.DataFrame({'date': ['2024-01-01'], 'title': ['SKí•˜ì´ë‹‰ìŠ¤ HBM3E'], 'content': ['SKí•˜ì´ë‹‰ìŠ¤ê°€ HBM3Eë¥¼ ì—”ë¹„ë””ì•„ì— ë…ì  ê³µê¸‰í•œë‹¤.']})\n",
    "    \n",
    "    sam_df['company'] = 'Samsung'\n",
    "    sk_df['company'] = 'SKHynix'\n",
    "    df = pd.concat([sam_df, sk_df], ignore_index=True)\n",
    "    df['text'] = df['title'].fillna('') + \" \" + df['content'].fillna('')\n",
    "    df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
    "    df = df.dropna(subset=['date']).sort_values('date')\n",
    "    \n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    df['processed_text'] = df['text'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "df = load_and_preprocess()\n",
    "print(f\"Total Data: {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 23:10:28,502 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERTopic...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8950bfe6616044a5b930beaf18290735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 23:10:36,496 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-23 23:10:36,497 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-23 23:10:48,464 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-23 23:10:48,465 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-23 23:10:48,498 - BERTopic - Cluster - Completed âœ“\n",
      "2025-11-23 23:10:48,501 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-23 23:10:49,449 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Training Complete.\n",
      "   Topic  Count                     Name  \\\n",
      "0     -1     13  -1_ì‚¼ì„±ì€_í¬ë§ë””ë”¤ëŒ_ì‚¼ì„±í¬ë§ë””ë”¤ëŒ_ì‚¼ì„±   \n",
      "1      0    937       0_ë°˜ë„ì²´_ìˆëŠ”_ai_skí•˜ì´ë‹‰ìŠ¤   \n",
      "2      1     50            1_ì œí’ˆ_ëŒ€ë¹„_ë§¤ì¶œ_ìˆ˜ìš”   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [ì‚¼ì„±ì€, í¬ë§ë””ë”¤ëŒ, ì‚¼ì„±í¬ë§ë””ë”¤ëŒ, ì‚¼ì„±, êµìœ¡, ëª…ì¥, ì±„ìš©, ê³µì±„, csr,...   \n",
      "1  [ë°˜ë„ì²´, ìˆëŠ”, ai, skí•˜ì´ë‹‰ìŠ¤, ë©”ëª¨ë¦¬, ê¸°ìˆ , í•¨ê»˜, ë‹¤ì–‘í•œ, ì‚¼ì„±ì „ì, ìœ„í•œ]   \n",
      "2     [ì œí’ˆ, ëŒ€ë¹„, ë§¤ì¶œ, ìˆ˜ìš”, íŒë§¤, ì‹¤ì , ì „ë¶„ê¸°, í”„ë¦¬ë¯¸ì—„, ì‹œì¥, ì‚¼ì„±ì „ìëŠ”]   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [ì²­ë…„ê³¼ í•¨ê»˜í•˜ëŠ” í–‰ë³µí•œ ë™í–‰, ì‚¼ì„±í¬ë§ë””ë”¤ëŒ ëŒ€ì „ì„¼í„° ê°œì†Œì‹ ì´ˆì—¬ë¦„ì˜ í‘¸ë¦‡í•œ ê¸°ìš´...  \n",
      "1  [[SKí•˜ì´ë‹‰ìŠ¤ 41ì£¼ë…„] â€œë¹›ë‚˜ëŠ” 40+1â€ 40ë…„ ê¸°ìˆ ë ¥ ë‹¤ì ¸ No.1ìœ¼ë¡œ ìš°ëš...  \n",
      "2  [ì‚¼ì„±ì „ì, 2023ë…„ 4ë¶„ê¸° ì‹¤ì  ë°œí‘œ ì‚¼ì„±ì „ìëŠ” ì—°ê²° ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¶œ 67.78ì¡°...  \n"
     ]
    }
   ],
   "source": [
    "# 2. BERTopic Analysis\n",
    "def run_bertopic(docs, min_topic_size=30):\n",
    "    print(\"Initializing BERTopic...\")\n",
    "    korean_stopwords = ['ê¸°ì', 'ë°°í¬', 'ê¸ˆì§€', 'ë¬´ë‹¨', 'ì „ì¬', 'ìœ„í•´', 'í†µí•´', 'ê´€ë ¨', 'ëŒ€í•œ', 'ì´ë²ˆ', 'ìˆë‹¤', 'í–ˆë‹¤', 'ë°í˜”ë‹¤']\n",
    "    vectorizer_model = CountVectorizer(stop_words=korean_stopwords)\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        min_topic_size=min_topic_size,\n",
    "        verbose=True\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(docs)\n",
    "    return topic_model, topics\n",
    "\n",
    "sample_df = df.tail(1000).copy()\n",
    "docs = sample_df['processed_text'].tolist()\n",
    "\n",
    "try:\n",
    "    topic_model, topics = run_bertopic(docs)\n",
    "    print(\"BERTopic Training Complete.\")\n",
    "    freq = topic_model.get_topic_info(); \n",
    "    print(freq.head(5))\n",
    "except Exception as e:\n",
    "    print(f\"BERTopic Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networks Built.\n"
     ]
    }
   ],
   "source": [
    "# 3. Semantic Network Analysis (SNA)\n",
    "def extract_keywords_regex(text):\n",
    "    if not isinstance(text, str): return []\n",
    "    tech_terms = re.findall(r'\\b[A-Za-z][A-Za-z0-9]+\\b', text)\n",
    "    korean_nouns = re.findall(r'[ê°€-í£]{2,}', text)\n",
    "    target_keywords = set(['HBM', 'DDR', 'NAND', 'AI', 'ë°˜ë„ì²´', 'íŒŒìš´ë“œë¦¬', 'ìˆ˜ìœ¨', 'íˆ¬ì', 'ì–‘ì‚°', \n",
    "                          'ì—”ë¹„ë””ì•„', 'TSMC', 'GAA', 'ì´ìµ', 'ì ì', 'CXL', 'PIM', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì‚¼ì„±ì „ì'])\n",
    "    found = []\n",
    "    for word in tech_terms + korean_nouns:\n",
    "        w_upper = word.upper()\n",
    "        for target in target_keywords:\n",
    "            if target in w_upper or w_upper in target:\n",
    "                found.append(target)\n",
    "                break\n",
    "    return list(set(found))\n",
    "\n",
    "def draw_semantic_network(df, company_name, threshold=5):\n",
    "    subset = df[df['company'] == company_name].copy()\n",
    "    subset['keywords'] = subset['processed_text'].apply(extract_keywords_regex)\n",
    "    edge_list = []\n",
    "    for keywords in subset['keywords']:\n",
    "        if len(keywords) > 1:\n",
    "            edge_list.extend(combinations(sorted(keywords), 2))\n",
    "    edge_counts = Counter(edge_list)\n",
    "    G = nx.Graph()\n",
    "    for (u, v), count in edge_counts.items():\n",
    "        if count >= threshold: G.add_edge(u, v, weight=count)\n",
    "    return G\n",
    "\n",
    "G_sk = draw_semantic_network(sample_df, \"SKHynix\", threshold=20)\n",
    "G_sam = draw_semantic_network(sample_df, \"Samsung\", threshold=20)\n",
    "print(\"Networks Built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    [ì—­í• ]\n",
      "    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µì°°ë ¥ ìˆëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
      "\n",
      "    [ë¶„ì„ ë°ì´í„°: SKí•˜ì´ë‹‰ìŠ¤]\n",
      "    1. í•µì‹¬ ì—°ê²° í‚¤ì›Œë“œ (Top 5): ['ë°˜ë„ì²´', 'SKí•˜ì´ë‹‰ìŠ¤', 'AI', 'HBM', 'ì–‘ì‚°']\n",
      "    2. ë„¤íŠ¸ì›Œí¬ íŠ¹ì§•: 'ë°˜ë„ì²´' í‚¤ì›Œë“œê°€ ì¤‘ì‹¬ í—ˆë¸Œ ì—­í• ì„ í•˜ë©° ê¸°ìˆ  ìƒíƒœê³„ë¥¼ ì£¼ë„í•¨.\n",
      "\n",
      "    [ìš”ì²­ ì‚¬í•­]\n",
      "    1. **ì „ëµ ì§„ë‹¨:** í˜„ì¬ SKí•˜ì´ë‹‰ìŠ¤ì˜ ê¸°ìˆ  ì „ëµì´ 'ì§‘ì¤‘í˜•'ì¸ì§€ 'ë¶„ì‚°í˜•'ì¸ì§€ ì§„ë‹¨í•´ ì£¼ì„¸ìš”.\n",
      "    2. **ì„±ê³¼ ì˜ˆì¸¡:** íŠ¹íˆ 'ë°˜ë„ì²´' ê¸°ìˆ ì´ AI ì‚°ì—… íŠ¸ë Œë“œì™€ ê²°í•©í•˜ì—¬ í–¥í›„ ì–´ë–¤ ì¬ë¬´ì  ì„±ê³¼ë¥¼ ë‚¼ì§€ ì¶”ë¡ í•´ ì£¼ì„¸ìš”.\n",
      "    3. **ì•½í•œ ì‹ í˜¸(Weak Signal):** ë„¤íŠ¸ì›Œí¬ ì£¼ë³€ë¶€ì— ìœ„ì¹˜í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì°¨ì„¸ëŒ€ ê¸°ìˆ ì„ ì œì•ˆí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# 4. LLM Prompt Generation\n",
    "def generate_llm_prompt(G, company_name):\n",
    "    degree_dict = dict(G.degree(weight='weight'))\n",
    "    top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_keywords = [k[0] for k in top_nodes]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    [ì—­í• ]\n",
    "    ë‹¹ì‹ ì€ ë°˜ë„ì²´ ì‚°ì—… ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µì°°ë ¥ ìˆëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
    "    \n",
    "    [ë¶„ì„ ë°ì´í„°: {company_name}]\n",
    "    1. í•µì‹¬ ì—°ê²° í‚¤ì›Œë“œ (Top 5): {top_keywords}\n",
    "    2. ë„¤íŠ¸ì›Œí¬ íŠ¹ì§•: '{top_keywords[0]}' í‚¤ì›Œë“œê°€ ì¤‘ì‹¬ í—ˆë¸Œ ì—­í• ì„ í•˜ë©° ê¸°ìˆ  ìƒíƒœê³„ë¥¼ ì£¼ë„í•¨.\n",
    "    \n",
    "    [ìš”ì²­ ì‚¬í•­]\n",
    "    1. **ì „ëµ ì§„ë‹¨:** í˜„ì¬ {company_name}ì˜ ê¸°ìˆ  ì „ëµì´ 'ì§‘ì¤‘í˜•'ì¸ì§€ 'ë¶„ì‚°í˜•'ì¸ì§€ ì§„ë‹¨í•´ ì£¼ì„¸ìš”.\n",
    "    2. **ì„±ê³¼ ì˜ˆì¸¡:** íŠ¹íˆ '{top_keywords[0]}' ê¸°ìˆ ì´ AI ì‚°ì—… íŠ¸ë Œë“œì™€ ê²°í•©í•˜ì—¬ í–¥í›„ ì–´ë–¤ ì¬ë¬´ì  ì„±ê³¼ë¥¼ ë‚¼ì§€ ì¶”ë¡ í•´ ì£¼ì„¸ìš”.\n",
    "    3. **ì•½í•œ ì‹ í˜¸(Weak Signal):** ë„¤íŠ¸ì›Œí¬ ì£¼ë³€ë¶€ì— ìœ„ì¹˜í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì°¨ì„¸ëŒ€ ê¸°ìˆ ì„ ì œì•ˆí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "target_prompt = generate_llm_prompt(G_sk, \"SKí•˜ì´ë‹‰ìŠ¤\")\n",
    "print(target_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI Config Check:\n",
      "- Endpoint: https://hspar-m7k2pfor-swedencentral.openai.azure.com/\n",
      "- Deployment Name: gpt-4o\n",
      "- API Version: 2024-12-01-preview\n",
      "ğŸ¤– Azure LLMì—ê²Œ ë¶„ì„ ìš”ì²­ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
      "\n",
      "==================== Azure LLM ë¶„ì„ ë¦¬í¬íŠ¸ ====================\n",
      "\n",
      "### SKí•˜ì´ë‹‰ìŠ¤ì˜ ê¸°ìˆ  ì „ëµ ë° ë¯¸ë˜ ì „ë§ ë³´ê³ ì„œ\n",
      "\n",
      "#### 1. **ì „ëµ ì§„ë‹¨: ì§‘ì¤‘í˜• ì „ëµ**\n",
      "SKí•˜ì´ë‹‰ìŠ¤ì˜ í•µì‹¬ ì—°ê²° í‚¤ì›Œë“œì™€ ë„¤íŠ¸ì›Œí¬ íŠ¹ì§•ì„ ë¶„ì„í•œ ê²°ê³¼, 'ë°˜ë„ì²´' í‚¤ì›Œë“œê°€ ì¤‘ì‹¬ í—ˆë¸Œ ì—­í• ì„ í•˜ë©° ê¸°ìˆ  ìƒíƒœê³„ë¥¼ ì£¼ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” SKí•˜ì´ë‹‰ìŠ¤ê°€ íŠ¹ì • í•µì‹¬ ê¸°ìˆ ì— ì§‘ì¤‘í•˜ì—¬ ê°€ì¹˜ë¥¼ ê·¹ëŒ€í™”í•˜ëŠ” **ì§‘ì¤‘í˜• ì „ëµ**ì„ ì±„íƒí•˜ê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "\n",
      "íŠ¹íˆ, 'HBM(High Bandwidth Memory)'ê³¼ 'AI'ì™€ì˜ ì—°ê²°ì„±ì€ SKí•˜ì´ë‹‰ìŠ¤ê°€ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ì˜ ê³ ì„±ëŠ¥Â·ê³ íš¨ìœ¨í™”ì— ì—­ëŸ‰ì„ ì§‘ì¤‘í•˜ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. HBMì€ AIì™€ ê³ ì„±ëŠ¥ ì»´í“¨íŒ…(HPC) ë¶„ì•¼ì—ì„œ í•„ìˆ˜ì ì¸ ê¸°ìˆ ë¡œ ë– ì˜¤ë¥´ê³  ìˆìœ¼ë©°, SKí•˜ì´ë‹‰ìŠ¤ëŠ” ì´ëŸ¬í•œ ë¯¸ë˜ ì„±ì¥ ë™ë ¥ ë¶„ì•¼ì— ê°•ë ¥íˆ íˆ¬ìí•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. ì´ ì „ëµì€ ê²½ìŸì‚¬ì™€ ì°¨ë³„í™”ëœ ê¸°ìˆ  ë¦¬ë”ì‹­ í™•ë³´ì™€ ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€ë¥¼ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "#### 2. **ì„±ê³¼ ì˜ˆì¸¡: ë°˜ë„ì²´ ê¸°ìˆ ê³¼ AI ì‚°ì—…ì˜ ê²°í•©**\n",
      "AI ì‚°ì—…ì˜ ê¸‰ê²©í•œ ì„±ì¥ê³¼ í•¨ê»˜ ë°˜ë„ì²´, íŠ¹íˆ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ì˜ ì¤‘ìš”ì„±ì´ ë”ìš± ë¶€ê°ë˜ê³  ìˆìŠµë‹ˆë‹¤. AI ì‹œìŠ¤í…œì€ ë°©ëŒ€í•œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê³ ì†Â·ê³ ì„±ëŠ¥ ë©”ëª¨ë¦¬ ê¸°ìˆ ì„ í•„ìš”ë¡œ í•˜ë©°, HBMì€ ì´ë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” í•µì‹¬ ê¸°ìˆ ë¡œ ìë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. SKí•˜ì´ë‹‰ìŠ¤ëŠ” HBM3ì„ í¬í•¨í•œ ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ì œí’ˆì„ ì–‘ì‚° ì¤‘ì´ë©°, ì´ë¥¼ í†µí•´ AIì™€ HPC ë¶„ì•¼ì—ì„œ í­ë°œì ì¸ ìˆ˜ìš” ì¦ê°€ë¥¼ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**ì¬ë¬´ì  ì„±ê³¼ ì „ë§**:\n",
      "- **ë§¤ì¶œ ì¦ê°€**: AI ì‚°ì—…ì˜ í™•ì¥ì€ ë°˜ë„ì²´ ìˆ˜ìš”ë¥¼ ê²¬ì¸í•˜ë©°, SKí•˜ì´ë‹‰ìŠ¤ ì œí’ˆêµ°ì˜ ë§¤ì¶œ ìƒìŠ¹ìœ¼ë¡œ ì´ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤. íŠ¹íˆ, AI ì„œë²„ì™€ í´ë¼ìš°ë“œ ì»´í“¨íŒ… ë¶„ì•¼ì—ì„œ HBMì˜ ìˆ˜ìš”ëŠ” ê¾¸ì¤€íˆ ì¦ê°€í•  ê²ƒìœ¼ë¡œ ë³´ì´ë©°, ì´ëŠ” ì—°í‰ê·  10~15%ì˜ ë§¤ì¶œ ì„±ì¥ë¥ ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ìˆ˜ìµì„± ê°œì„ **: HBMê³¼ ê°™ì€ ê³ ë¶€ê°€ê°€ì¹˜ ì œí’ˆì€ í‰ê·  íŒë§¤ ê°€ê²©(ASP)ì„ ëŒì–´ì˜¬ë ¤ SKí•˜ì´ë‹‰ìŠ¤ì˜ ì „ë°˜ì ì¸ ìˆ˜ìµì„±ì„ ê°œì„ í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "- **ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€**: NVIDIA, AMD ë“± AI ì¹© ì œì¡°ì‚¬ì™€ì˜ í˜‘ë ¥ ê°•í™”ëŠ” SKí•˜ì´ë‹‰ìŠ¤ê°€ ì‹œì¥ ë‚´ ì§€ë°°ë ¥ì„ ë”ìš± í™•ëŒ€í•˜ëŠ” ë° ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "#### 3. **ì•½í•œ ì‹ í˜¸(Weak Signal): ì°¨ì„¸ëŒ€ ê¸°ìˆ  ì œì•ˆ**\n",
      "SKí•˜ì´ë‹‰ìŠ¤ì˜ ê¸°ìˆ  ë„¤íŠ¸ì›Œí¬ì—ì„œ ì£¼ë³€ë¶€ì— ìœ„ì¹˜í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì°¨ì„¸ëŒ€ ê¸°ìˆ ì€ **CMOS ì´ë¯¸ì§€ ì„¼ì„œ(CIS)**ì™€ **MRAM(Magnetoresistive RAM)**ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì´ ë‘ ê¸°ìˆ ì˜ ì ì¬ì„±ê³¼ SKí•˜ì´ë‹‰ìŠ¤ê°€ ì´ë¥¼ ì£¼ëª©í•´ì•¼ í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤.\n",
      "\n",
      "1. **CMOS ì´ë¯¸ì§€ ì„¼ì„œ(CIS)**:\n",
      "   - **ì ì¬ì„±**: CMOS ì´ë¯¸ì§€ ì„¼ì„œëŠ” ìŠ¤ë§ˆíŠ¸í°, ììœ¨ì£¼í–‰ì°¨, IoT ë””ë°”ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë©°, íŠ¹íˆ AI ê¸°ë°˜ ë¹„ì „ ì‹œìŠ¤í…œê³¼ ê²°í•©í•  ë•Œ ê°•ë ¥í•œ ì‹œë„ˆì§€ë¥¼ ë°œíœ˜í•©ë‹ˆë‹¤.\n",
      "   - **ì‚°ì—… ë™í–¥**: AIì™€ ì—£ì§€ ì»´í“¨íŒ…ì´ ì„±ì¥í•˜ë©´ì„œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ì´ ì¦ê°€í•˜ê³  ìˆì–´ CISì˜ ìˆ˜ìš”ê°€ í™•ëŒ€ë  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n",
      "   - **SKí•˜ì´ë‹‰ìŠ¤ì˜ ê¸°íšŒ**: ê¸°ì¡´ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ CIS ê°œë°œ ë° ìƒì‚° íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ìƒˆë¡œìš´ ë§¤ì¶œì› í™•ë³´ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **MRAM (Magnetoresistive RAM)**:\n",
      "   - **ì ì¬ì„±**: MRAMì€ ë¹„íœ˜ë°œì„± ë©”ëª¨ë¦¬ë¡œ, ë†’ì€ ì†ë„ì™€ ë‚®ì€ ì „ë ¥ ì†Œë¹„ë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•©ë‹ˆë‹¤. AIì™€ IoT ê¸°ê¸°ì—ì„œ ì—ë„ˆì§€ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ë§Œí¼ MRAMì€ ì°¨ì„¸ëŒ€ ë©”ëª¨ë¦¬ ì†”ë£¨ì…˜ìœ¼ë¡œ ë¶€ìƒí•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "   - **ì‚°ì—… ë™í–¥**: MRAMì€ ì•„ì§ ì´ˆê¸° ë‹¨ê³„ì— ìˆìœ¼ë‚˜, ë°ì´í„° ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í™•ëŒ€ì™€ í•¨ê»˜ ë¯¸ë˜ í•µì‹¬ ë©”ëª¨ë¦¬ ê¸°ìˆ ë¡œ ìë¦¬ì¡ì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n",
      "   - **SKí•˜ì´ë‹‰ìŠ¤ì˜ ê¸°íšŒ**: SKí•˜ì´ë‹‰ìŠ¤ëŠ” MRAM ê°œë°œì— ì¼ì°ì´ íˆ¬ìí•˜ì—¬ ê¸°ìˆ  ì„ ì ì„ ë…¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ê²½ìŸì‚¬ ëŒ€ë¹„ ì°¨ë³„í™”ëœ ê¸°ìˆ  í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "#### 4. **ê²°ë¡  ë° ì œì–¸**\n",
      "SKí•˜ì´ë‹‰ìŠ¤ëŠ” í˜„ì¬ 'ì§‘ì¤‘í˜•' ê¸°ìˆ  ì „ëµì„ í†µí•´ HBMê³¼ ê°™ì€ ê³ ë¶€ê°€ê°€ì¹˜ ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì‹œì¥ì„ ê³µëµí•˜ë©°, AI ì‚°ì—…ì˜ ì„±ì¥ê³¼ í•¨ê»˜ ë†’ì€ ë§¤ì¶œ ì¦ê°€ ë° ìˆ˜ìµì„±ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ìˆ  ìƒíƒœê³„ì—ì„œ ì£¼ë³€ë¶€ì— ìˆëŠ” ì•½í•œ ì‹ í˜¸(Weak Signal)ë¥¼ í¬ì°©í•˜ì—¬ ë¯¸ë˜ì„±ì¥ ê°€ëŠ¥ì„±ì„ ì¡°ê¸°ì— í™•ë³´í•˜ëŠ” ê²ƒë„ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ì— ë”°ë¼ SKí•˜ì´ë‹‰ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì „ëµì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:\n",
      "1. **HBM ê¸°ìˆ  ê³ ë„í™”**: ì§€ì†ì ì¸ R&D íˆ¬ìì™€ ì–‘ì‚° ì—­ëŸ‰ ê°•í™”ë¡œ AI ë° HPC ì‹œì¥ ë‚´ í™•ê³ í•œ ì§€ìœ„ ìœ ì§€.\n",
      "2. **ì°¨ì„¸ëŒ€ ê¸°ìˆ  ë°œêµ´**: CMOS ì´ë¯¸ì§€ ì„¼ì„œ ë° MRAMê³¼ ê°™ì€ ì£¼ë³€ë¶€ ê¸°ìˆ ì— ëŒ€í•œ ì„ ì œì  íˆ¬ìë¡œ ì¥ê¸°ì  ì„±ì¥ ê¸°ë°˜ ë§ˆë ¨.\n",
      "3. **AI í˜‘ë ¥ ê°•í™”**: NVIDIA, AMD ë“± AI ì¹© ì œì¡°ì‚¬ì™€ì˜ í˜‘ë ¥ì„ í™•ëŒ€í•˜ì—¬ ë°˜ë„ì²´ ìƒíƒœê³„ ë‚´ ì—­í• ì„ ê°•í™”.\n",
      "\n",
      "ì´ì™€ ê°™ì€ ì „ëµì  ë°©í–¥ì€ SKí•˜ì´ë‹‰ìŠ¤ê°€ AI ê¸°ìˆ  íŠ¸ë Œë“œì™€ ë°˜ë„ì²´ ì‹œì¥ì˜ ê¸‰ë³€í•˜ëŠ” í™˜ê²½ì—ì„œ ì§€ì† ê°€ëŠ¥í•˜ê³  ì•ˆì •ì ì¸ ì„±ì¥ì„ ì´ë£¨ëŠ” ë° ê¸°ì—¬í•  ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 5. Azure OpenAI Integration\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ë””ë²„ê¹…ìš©, ì‹¤ì œ í‚¤ ê°’ì€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ)\n",
    "print(\"Azure OpenAI Config Check:\")\n",
    "print(f\"- Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"- Deployment Name: {os.getenv('AZURE_OPENAI_LLM_DEPLOYMENT')}\")\n",
    "print(f\"- API Version: {os.getenv('AZURE_OPENAI_API_VERSION')}\")\n",
    "\n",
    "def query_azure_llm(prompt):\n",
    "    # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "        api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "        api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    )\n",
    "    \n",
    "    deployment_name = os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\") # e.g., 'gpt-4o'\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ¤– Azure LLMì—ê²Œ ë¶„ì„ ìš”ì²­ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled in semiconductor industry analysis.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error calling Azure OpenAI: {e}\"\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ (í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì‹¤í–‰ë¨)\n",
    "if os.getenv('AZURE_OPENAI_API_KEY'):\n",
    "    analysis_result = query_azure_llm(target_prompt)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*20 + \" Azure LLM ë¶„ì„ ë¦¬í¬íŠ¸ \" + \"=\"*20 + \"\\n\")\n",
    "    print(analysis_result)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ .env íŒŒì¼ì— AZURE_OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
